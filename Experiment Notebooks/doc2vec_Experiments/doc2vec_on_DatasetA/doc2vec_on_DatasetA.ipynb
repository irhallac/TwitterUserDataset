{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# please change min_count=5, dm=0 parameters as you wish to replicate an experiment\n",
    "# train test split is  %60 to %40 \n",
    "\n",
    "data_path = \"DATASTORE/DatasetA/corpus\"  #where training data are (corpus)\n",
    "path_train = \"DATASTORE/DatasetA/train\"  # where training user tweets are\n",
    "path_test = \"DATASTORE/DatasetA/test\"    # where training user tweets are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "endItr = 100  #last iteration \n",
    "max_epochs = endItr\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "workers = cpu_count() * 0.75\n",
    "\n",
    "vec_size = 100\n",
    "alpha = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vol/ibrahim/.conda/envs/py37irh/bin/python\n"
     ]
    }
   ],
   "source": [
    "#Import all  dependencies\n",
    "from sklearn import metrics\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime \n",
    "from scipy import spatial\n",
    "#result = 1 - spatial.distance.cosine(v1, v2)\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "here = os.getcwd()\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "from reviewFunctions import user2vec_test\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename=\"doc2vec_DatasetA.log\",\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.CRITICAL)\n",
    "\n",
    "logging.critical(\"\\n doc2vec_DatasetA experiment 1\")\n",
    "logger = logging.getLogger('doc2vec_DatasetA')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "#Data cleaning\n",
    "def clean_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text)\n",
    "    word_list = []\n",
    "    for i in text:      \n",
    "        if (i.isalnum() and (not i in stop_words)):\n",
    "            word_list += [i]\n",
    "        \n",
    "    return word_list\n",
    "\n",
    "def docIter(data_path):\n",
    "    i = -1\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            file_path = os.path.join(root, file)\n",
    "            document = open(file_path, encoding=\"utf-8\").readlines()\n",
    "            for line in document:\n",
    "                i += 1\n",
    "                yield gensim.models.doc2vec.TaggedDocument(clean_tokenize(line), [i])\n",
    "\n",
    "def get_user_dataframe(path):\n",
    "    dataFrames = []\n",
    "    for folder in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, folder)):\n",
    "            print(\"folder: \", folder)\n",
    "            for file_name in os.listdir(os.path.join(path, folder)):\n",
    "                if file_name.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(path, folder, file_name)\n",
    "                    df = pd.read_csv(file_path,header=None,usecols=[3,0,2], \n",
    "                                     names=['tweet_id', 'date', 'user_name', 'text'])\n",
    "                    df = df.astype(str)\n",
    "                    df[\"category\"] = folder\n",
    "                    dataFrames.append(df)\n",
    "\n",
    "    dfs = pd.concat(dataFrames)\n",
    "    print(\"total \", len(dfs), \" tweets\")\n",
    "    return dfs\n",
    "\n",
    "\n",
    "\n",
    "#saveJson(path_prefix, user_vecs, ctg_vecs, self.epoch) changedd\n",
    "def saveJson(path_prefix, user_vecs_test, ctg_vecs_train, user_vecs_train, epoch):\n",
    "    logging.critical(\"saveJSon started\")\n",
    "    path = get_tmpfile('{}/embeddings/users_test{}.json'.format(path_prefix,epoch))\n",
    "    with open(path, 'w') as outfile:\n",
    "        json.dump(user_vecs_test, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    path = get_tmpfile('{}/embeddings/categories_train{}.json'.format(path_prefix,epoch))\n",
    "    with open(path, 'w') as outfile:\n",
    "        json.dump(ctg_vecs_train, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    path = get_tmpfile('{}/embeddings/users_train{}.json'.format(path_prefix,epoch))\n",
    "    with open(path, 'w') as outfile:\n",
    "        json.dump(user_vecs_train, outfile)\n",
    "    outfile.close()\n",
    "    logging.critical(\"saveJSon finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasetA_tweetOnly.txt\n",
      "folder:  twcollector3\n",
      "folder:  twcollector5\n",
      "folder:  twcollector1\n",
      "folder:  twcollector2\n",
      "folder:  twcollector4\n",
      "total  300000  tweets\n",
      "folder:  twcollector3\n",
      "folder:  twcollector5\n",
      "folder:  twcollector1\n",
      "folder:  twcollector2\n",
      "folder:  twcollector4\n",
      "total  200000  tweets\n"
     ]
    }
   ],
   "source": [
    "train_data = list(docIter(data_path))\n",
    "dfs_train = get_user_dataframe(path_train)\n",
    "dfs_test = get_user_dataframe(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iter_acc(model, dfs_train, dfs_test, itr):\n",
    "    # creaa\n",
    "    user_vecs_train = {}\n",
    "    ctg_vecs_train = {}\n",
    "    user_vecs_test = {}\n",
    "  \n",
    "    logging.critical(\"beginning: of calc_iter_acc\")\n",
    "    logging.critical(\"first: inferring user_vecs_train\")\n",
    "    for index, datapoint in dfs_train.iterrows():\n",
    "        unname = datapoint[\"user_name\"]\n",
    "        tid = datapoint[\"tweet_id\"]\n",
    "        category = datapoint[\"category\"]\n",
    "        tokenized_words = clean_tokenize(datapoint[\"text\"])\n",
    "        vec = model.infer_vector(tokenized_words)\n",
    "        if unname in user_vecs_train.keys():\n",
    "            user_vecs_train[unname][\"vecs\"].append(vec)\n",
    "        else:\n",
    "            user_vecs_train[unname] = {\"vecs\": [vec], \"category\": category}\n",
    "\n",
    "    # find the average of tweet vectors for each user\n",
    "    for unm in user_vecs_train.keys():\n",
    "        user_vecs_train[unm][\"avr_vec\"] = np.average(np.array(user_vecs_train[unm][\"vecs\"]), axis=0)              \n",
    "      \n",
    "    #\n",
    "    logging.critical(\"second: inferring user_vecs_test\")\n",
    "    for index, datapoint in dfs_test.iterrows():\n",
    "        unname = datapoint[\"user_name\"]\n",
    "        tid = datapoint[\"tweet_id\"]\n",
    "        category = datapoint[\"category\"]\n",
    "        tokenized_words = clean_tokenize(datapoint[\"text\"])\n",
    "        vec = model.infer_vector(tokenized_words)\n",
    "        if unname in user_vecs_test.keys():\n",
    "            user_vecs_test[unname][\"vecs\"].append(vec)\n",
    "        else:\n",
    "            user_vecs_test[unname] = {\"vecs\": [vec], \"category\": category}\n",
    "\n",
    "    # find the average of tweet vectors for each user, dfs_test\n",
    "    for unm in user_vecs_test.keys():\n",
    "        user_vecs_test[unm][\"avr_vec\"] = np.average(np.array(user_vecs_test[unm][\"vecs\"]), axis=0)\n",
    "    #\n",
    "    \n",
    "    # create category vector dictionary\n",
    "    for unm in user_vecs_train.keys():\n",
    "        avg = user_vecs_train[unm][\"avr_vec\"]\n",
    "        ctg = user_vecs_train[unm][\"category\"]\n",
    "        if ctg in ctg_vecs_train.keys():\n",
    "            ctg_vecs_train[ctg][\"cat_vecs\"].append(avg)\n",
    "        else:\n",
    "            ctg_vecs_train[ctg] = {\"cat_vecs\": [avg]}\n",
    "\n",
    "    # find the average of category vectors\n",
    "    for ctg in ctg_vecs_train.keys():\n",
    "        ctg_vecs_train[ctg][\"avr_cat_vec\"] = np.average(np.array(ctg_vecs_train[ctg][\"cat_vecs\"]), axis=0)  \n",
    "    \n",
    "    users_test = {}\n",
    "    for usr in user_vecs_test.keys():\n",
    "        users_test[usr] = {'avr_vec' : user_vecs_test[usr][\"avr_vec\"].tolist(), 'category': user_vecs_test[usr][\"category\"]}\n",
    "    \n",
    "    users_train = {}\n",
    "    for usr in user_vecs_train.keys():\n",
    "        users_train[usr] = {'avr_vec' : user_vecs_train[usr][\"avr_vec\"].tolist(), 'category': user_vecs_train[usr][\"category\"]}\n",
    "    \n",
    "    \n",
    "    categories_train = {}\n",
    "    for ctg in ctg_vecs_train.keys():\n",
    "        categories_train[ctg] = {'avr_cat_vec' : ctg_vecs_train[ctg][\"avr_cat_vec\"].tolist()}\n",
    "    \n",
    "    logging.critical(\"end of: calc_iter_acc\")\n",
    "    uvt = user2vec_test(user_vecs=users_test, ctg_vecs=categories_train) # test model\n",
    "    msg1 = uvt.calc_accuracy()\n",
    "    logging.critical(msg1)\n",
    "    msg2 = uvt.calc_accuracy_by_group()\n",
    "    logging.critical(msg2)\n",
    "   \n",
    "    #conf matrix\n",
    "    true_labels = []\n",
    "    predicted_labes = []\n",
    "\n",
    "    for unm in uvt.user_vecs.keys():\n",
    "        true_labels.append(uvt.getUserCategory(unm))\n",
    "        predicted_labes.append(uvt.most_similar_group(unm))\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    msg3 = metrics.confusion_matrix(true_labels, predicted_labes)\n",
    "    logging.critical(msg3)\n",
    "\n",
    "    # Print the precision and recall, among other metrics\n",
    "    msg4 = metrics.classification_report(true_labels, predicted_labes, digits=3)\n",
    "    logging.critical(msg4)\n",
    "    \n",
    "    subject = \"doc2vec GPU2 alert - Epoch_\"+ str(itr) + \" Acc and Conf Matrix\"\n",
    "    #send_email(user, pwd, recipient, subject, msg1 + \"\\n\\n\" + msg2 + \"\\n\\n\" + msg4)     \n",
    "    return users_test, categories_train, users_train, uvt.acc\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "#'''Asses the model at each iteration.'''\n",
    "    def __init__(self, path_prefix):\n",
    "        print(\"__init-EpochSaver__ called\")\n",
    "        self.path_prefix = path_prefix\n",
    "        self.epoch = 1\n",
    "        self.best_acc = 0\n",
    "        self.best_epoch = 1\n",
    "        msg = 'Epoch-{} started'.format(self.epoch)\n",
    "        logging.critical(msg)\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        msg = 'Epoch-{} finished \\n'.format(self.epoch)\n",
    "        logging.critical(msg)\n",
    "        # calc_iter_acc returns: users_test, categories_train, users_train, uvt.acc\n",
    "        user_vecs, ctg_vecs, user_vecs_train, tmp_acc  = calc_iter_acc(model, dfs_train, dfs_test, self.epoch)\n",
    "        if tmp_acc > self.best_acc:\n",
    "            msg = \"model improved as general acc  \\n\"\n",
    "            print(msg)\n",
    "            logging.critical(msg)\n",
    "            self.best_acc = tmp_acc\n",
    "            self.best_epoch = self.epoch            \n",
    "            model_path = get_tmpfile('{}/embeddings/epoch{}.model'.format(self.path_prefix, self.epoch))\n",
    "            model.save(model_path) \n",
    "        else:\n",
    "            msg = \"not saving the model Bc it didn't improve as general_acc  \\n\"\n",
    "            print(msg)\n",
    "            logging.critical(msg)             \n",
    "        #saveJson(path_prefix, user_vecs_test, ctg_vecs_train, user_vecs_train, epoch\n",
    "        saveJson(self.path_prefix, user_vecs, ctg_vecs, user_vecs_train, self.epoch)  \n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init-EpochSaver__ called\n"
     ]
    }
   ],
   "source": [
    "here = os.getcwd()\n",
    "epoch_saver = EpochSaver(path_prefix=here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=vec_size, min_count=5,alpha=alpha,min_alpha=0.00025,dm=0, \n",
    "                epochs=max_epochs, callbacks=[epoch_saver], workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model improved as general acc  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/ibrahim/.conda/envs/py37irh/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "print(\"End of training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.critical(\"End of the experiment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
