{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching a tweet dataset with Name, Location, Description of the author and handles of the users who Retweeted, Liked, Commented "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**path_dataSet_A** : previously created dataset consisting of 1K tweet_id and tweet pairs of 500 users\n",
    "\n",
    "**path_rtLikeCommentData** : previously created dataset consisting of lists of user handles of who liked, commented, and retweeted\n",
    "\n",
    "**path_enriched_dataSet_A**: path to save the final dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the following cells, 3 sample output from the dataset are given. If you check out the first record (i),\n",
    "\n",
    "> **tweet text is** \"A new year for artificial human intelligence  https://t.co/mXPCnsFnC6 #ArtificialIntelligence #ai #datascience #MachineLearning #automation #SelfDrivingCars #automation https://t.co/z6x08EeoEc\"\n",
    "\n",
    "> **user (author) handle is** \"JeffreyBuskey\"\n",
    "\n",
    "> **location is** \"Chicago\"\n",
    "\n",
    "> **description text is**  \"Building productive Sales teams one Company at a time while increasing sales growth using Social Media and Marketing | Leadership| Sales Coach | CyberSecurity\"\n",
    "\n",
    "> **name of the users who retweeted are** \"RobotConsumer\"\n",
    "\n",
    "> and **there are no likes or commentes**\n",
    "\n",
    "With the script given in this notebook, we combine these different information (tweet, user and user activity data) into a single record by addding dummy tokens between them. Tweet and Rt&Like&Comment are collected before. Name&Location&Description are collected on the fly by using the Twitter API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A new year for artificial human intelligence  https://t.co/mXPCnsFnC6 #ArtificialIntelligence #ai #datascience #MachineLearning #automation #SelfDrivingCars #automation https://t.co/z6x08EeoEc 2764enrtag0918  2764enrtag0918  2764enrtag0918  2764enrtag0918  2764enrtag0918  2764name0918  2764name0918  2764name0918  2764name0918  2764name0918  JeffreyBuskey  2764loc0918  2764loc0918  2764loc0918  2764loc0918  2764loc0918  Chicago  2764desc0918  2764desc0918  2764desc0918  2764desc0918  2764desc0918  Building productive Sales teams one Company at a time while increasing sales growth using Social Media and Marketing | Leadership| Sales Coach | CyberSecurity  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918  (hiç like yok)  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918  RobotConsumer  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764comment0918  2764comment0918  2764comment0918  2764comment0918  2764comment0918  (hiç comment yok)  2764comment0918  2764comment0918  2764comment0918  2764comment0918  2764comment0918"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is a service mesh, and how does it relate to networking? https://t.co/dhQfHNiAKx via @networkingtt 2764enrtag0918  2764enrtag0918  2764enrtag0918  2764enrtag0918  2764enrtag0918  2764name0918  2764name0918  2764name0918  2764name0918  2764name0918  leedoyle_dc  2764loc0918  2764loc0918  2764loc0918  2764loc0918  2764loc0918  Boston area  2764desc0918  2764desc0918  2764desc0918  2764desc0918  2764desc0918  Principal Analyst at Doyle Research - focusing on the Intelligent Network, SDN/OpenFlow, NFV, COTS, CSCO, JNPR.  Hike|Ski|Mtn Bike|Ultimate Frisbee  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918  (hiç like yok)  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918  (hiç rt yok)  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764comment0918  2764comment0918  2764comment0918  2764comment0918  2764comment0918  (hiç comment yok)  2764comment0918  2764comment0918  2764comment0918  2764comment0918  2764comment0918"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Merry Christmas and Happy Holidays to ALL. I hope your holiday is filled with excitement from family and friends @Paula_Piccard  @jerome_joffre @andi_staub @TopCyberNews @MHiesboeck @rajat_shrimal @kashthefuturist @helene_wpli @FrRonconi @FrRonconi @psb_dc @antgrasso https://t.co/DfDjPNJMFc 2764enrtag0918  2764enrtag0918  2764enrtag0918  2764enrtag0918  2764enrtag0918  2764name0918  2764name0918  2764name0918  2764name0918  2764name0918  JeffreyBuskey  2764loc0918  2764loc0918  2764loc0918  2764loc0918  2764loc0918  Chicago  2764desc0918  2764desc0918  2764desc0918  2764desc0918  2764desc0918  Building productive Sales teams one Company at a time while increasing sales growth using Social Media and Marketing | Leadership| Sales Coach | CyberSecurity  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918  psb_dc  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   smfmrm  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   isidoro63  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   MHcommunicate  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   RobertR41182121  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   88lfm88  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   kenguapp  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   rasulvatsan  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   DirkBuecker  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   antgrasso  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   UnlockBiz  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   Devtechno_Dubai  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   jacqartdata  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   AdamSmets1  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   TopCyberNews  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   helene_wpli  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   MasterofIoT  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   andi_staub  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   futurion3d  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   Paula_Piccard  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   jerome_joffre  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   rvworldnetwork  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918   kashthefuturist  2764like0918  2764like0918  2764like0918  2764like0918  2764like0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918  jacqartdata  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918   His5177  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918   SalvadorAvenda9  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918   futurion3d  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918   rvworldnetwork  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764rt0918  2764comment0918  2764comment0918  2764comment0918  2764comment0918  2764comment0918  chriskclark  2764comment0918  2764comment0918  2764comment0918  2764comment0918  2764comment0918   psb_dc  2764comment0918  2764comment0918  2764comment0918  2764comment0918  2764comment0918   helene_wpli  2764comment0918  2764comment0918  2764comment0918  2764comment0918  2764comment0918"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "**1) In our first attempt we had used the following words as dummy tokens:**\n",
    "\n",
    "> \\*&enr\\*&, \\*&name\\*&, \\*&loc\\*&, \\*&like\\*&, \\*&rt\\*&, \\*&comment\\*&, \\*&desc\\*&\n",
    "\n",
    "But we had found that they were considered as stop words (non-alphanumeric characters) in the text cleaning process. Therefore, we replaced them with the following words. These are the tokens that refer to \"enrtag\", \"nametag\", etc. in the paper. \n",
    "\n",
    "> _2764enrtag0918, 2764name0918, 2764loc0918, 2764like0918, 2764rt0918, 2764comment0918_\n",
    "\n",
    "By this way, we made sure that they were considered as valid tokens. The purpose of adding numbers (a random combination) \"2764-0918\" is to make them unluckly to apper in the original tweet text or user info.\n",
    "\n",
    "> _rpl -R \"\\*&enr\\*&\" \"2764enrtag0918\" #shell command for replacing words_\n",
    "\n",
    "\n",
    "**2) (hiç like yok) , (hiç rt yok), and (hiç comment yok)**\n",
    "\n",
    " These are used as additional dummy tokens when one of the user lists (like, comment, rt) is empty: \n",
    " <br> when there is no user who liked the tweet \"(hiç like yok)\" text is added\n",
    " <br> when there is no user who retweeted the tweet \"(hiç rt yok)\" text is added \n",
    " <br> when there is no user who commented on the tweet \"(hiç comment yok) \" text is added between the related dummy tokens.\n",
    " \n",
    " \"(hiç like yok)\" means \"there are no likes\" in Turkish.\n",
    " <br> \"(hiç comment yok)\" means \"there are no comments\" in Turkish..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import tweepy\n",
    "import re\n",
    "\n",
    "\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "\n",
    "# fill in the values with your own Twitter API cridentials.\n",
    "TWITTER_APP_KEY = \"\"\n",
    "TWITTER_APP_SECRET = \"\"\n",
    "TWITTER_KEY = \"\"\n",
    "TWITTER_SECRET = \"\"\n",
    "\n",
    "auth = OAuthHandler(TWITTER_APP_KEY, TWITTER_APP_SECRET)\n",
    "auth.set_access_token(TWITTER_KEY, TWITTER_SECRET)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "\n",
    "def createDir(dirPath):\n",
    "    if not os.path.exists(dirPath):\n",
    "        os.mkdir(dirPath)\n",
    "        \n",
    "        \n",
    "def userNameLocScrn(user_name):\n",
    "    jsonData ={}\n",
    "    user = api.get_user(user_name)\n",
    "    jsonData.update({\"loc\":user.location, \"scrn\":user.screen_name, \"desc\":user.description})\n",
    "    return jsonData\n",
    "\n",
    "\n",
    "def strValueUpdate(sentence, key, lookUp):\n",
    "    for i in range(0,6):\n",
    "        if lookUp!= \" *&enr*& \" and lookUp!=\"keyBitis\":\n",
    "            if i == 5 :\n",
    "                sentence += \" {} \".format(lookUp)\n",
    "            else:\n",
    "                sentence+=key\n",
    "        else:\n",
    "            if i<5:\n",
    "                sentence += key\n",
    "            else:\n",
    "                break\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def newCsvLine(oldList,newList,key_indis):\n",
    "    keyArr = [\" *&like*& \", \" *&rt*& \", \" *&comment*& \"]\n",
    "    lkRtCmmntNotFoundStr = [\"(hiç like yok)\", \"(hiç rt yok)\", \"(hiç comment yok)\"]\n",
    "    if len(newList)==0:\n",
    "        oldList = strValueUpdate(oldList, keyArr[key_indis],lkRtCmmntNotFoundStr[key_indis])\n",
    "    else:\n",
    "        for list_val in newList.split(','):\n",
    "            oldList = strValueUpdate(oldList, keyArr[key_indis],list_val)\n",
    "    oldList = strValueUpdate(oldList, keyArr[key_indis], \"keyBitis\")\n",
    "    return oldList\n",
    "\n",
    "\n",
    "def createEnrichedDataset(mainDir,newSaveDir,lRCTwtHpDir):\n",
    "    for dirName in os.listdir(mainDir):\n",
    "        newDirPath = os.path.join(newSaveDir,dirName)\n",
    "        createDir(newDirPath)\n",
    "        for subDirName in os.listdir(os.path.join(mainDir,dirName)):\n",
    "            newSubDirPath = os.path.join(newDirPath, subDirName)\n",
    "            createDir(newSubDirPath)\n",
    "            for subDirNameFileName in os.listdir(os.path.join(mainDir,dirName,subDirName)):\n",
    "                textFile = os.path.join(mainDir,dirName,subDirName,subDirNameFileName)\n",
    "                lrcFile = os.path.join(lRCTwtHpDir,subDirName, subDirNameFileName)\n",
    "                newSaveCsvFile =  os.path.join(newDirPath, subDirName,subDirNameFileName)\n",
    "                with open(textFile, 'r',encoding=\"mbcs\") as csvTextFile,open(lrcFile, 'r',encoding=\"mbcs\") as csvLRCFile:\n",
    "                    readerText = csv.reader(csvTextFile)\n",
    "                    readerLRC = csv.reader(csvLRCFile)\n",
    "                    for rowText in readerText:\n",
    "                        newliste = []\n",
    "                        newliste.append(rowText[0])\n",
    "                        newliste.append(rowText[1])\n",
    "                        newliste.append(rowText[2])\n",
    "                        oldTwt = rowText[3]\n",
    "                        for rowLRC in readerLRC:\n",
    "                            if rowLRC[0].split('{!!}')[0]==rowText[0]:\n",
    "                                locScrnDesc = userNameLocScrn(rowText[2])\n",
    "                                newTwt = strValueUpdate(oldTwt,\" *&enr*& \",\" *&enr*& \")\n",
    "\n",
    "                                newTwt = strValueUpdate(newTwt, \" *&name*& \",locScrnDesc['scrn'])\n",
    "\n",
    "                                newTwt = strValueUpdate(newTwt, \" *&loc*& \",locScrnDesc['loc'])\n",
    "\n",
    "                                newTwt = strValueUpdate(newTwt, \" *&desc*& \",locScrnDesc['desc'])\n",
    "\n",
    "                                listToStr = ', '.join(rowLRC)\n",
    "                                LkRtCmtIndex=[m.start() for m in re.finditer('{!!}',listToStr)]\n",
    "                                likeList = listToStr[LkRtCmtIndex[0]+4:LkRtCmtIndex[1]]\n",
    "                                rtList = listToStr[LkRtCmtIndex[1] + 4:LkRtCmtIndex[2]]\n",
    "                                comtList = listToStr[LkRtCmtIndex[2] + 4:]\n",
    "                                newTwt = newCsvLine(newTwt, likeList, 0)\n",
    "                                newTwt = newCsvLine(newTwt, rtList, 1)\n",
    "                                newTwt = newCsvLine(newTwt, comtList, 2)\n",
    "                                newliste.append(newTwt)\n",
    "                                with open(newSaveCsvFile,'a',encoding='utf-8') as wF:\n",
    "                                    writer = csv.writer(wF)\n",
    "                                    writer.writerow(newliste)\n",
    "\n",
    "                                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(os.getcwd(), \"path_dataSet_A\")\n",
    "output_path = os.path.join(os.getcwd(), \"path_enriched_dataSet_A\")\n",
    "rtLikeComment_path = os.path.join(os.getcwd(), \"path_rtLikeCommentData\")\n",
    "\n",
    "createEnrichedDataset(input_path, output_path, rtLikeComment_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
